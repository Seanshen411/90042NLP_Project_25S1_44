{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train claims: 1228\n",
            "Dev claims: 154\n",
            "Evidences: 1208827\n",
            "Test claims: 153\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "data_file = \"/content/data\"\n",
        "data_file = \"data\"\n",
        "train_file = os.path.join(data_file, 'train-claims.json')\n",
        "dev_file = os.path.join(data_file, 'dev-claims.json')\n",
        "evidence_file = os.path.join(data_file, 'evidence.json')\n",
        "test_file = os.path.join(data_file, 'test-claims-unlabelled.json')\n",
        "\n",
        "with open(train_file, 'r') as f:\n",
        "    tr_claims = json.load(f)\n",
        "tr_numbers = list(tr_claims.keys())\n",
        "tr_texts = [tr_claims[claim_id]['claim_text'] for claim_id in tr_numbers]\n",
        "claim_number_to_tr_id = {claim_id: i for i, claim_id in enumerate(tr_numbers)}\n",
        "\n",
        "with open(dev_file, 'r') as f:\n",
        "    dev_claims = json.load(f)\n",
        "dev_numbers = list(dev_claims.keys())\n",
        "dev_texts = [dev_claims[claim_id]['claim_text'] for claim_id in dev_numbers]\n",
        "\n",
        "with open(evidence_file, 'r') as f:\n",
        "    evidences = json.load(f)\n",
        "evi_numbers = list(evidences.keys())\n",
        "evidences_texts = [evidences[evidence_id] for evidence_id in evi_numbers]\n",
        "evi_number_to_evi_id = {evi_number: i for i, evi_number in enumerate(evi_numbers)}\n",
        "\n",
        "with open(test_file, 'r') as f:\n",
        "    test_claims = json.load(f)\n",
        "ts_numbers = list(test_claims.keys())\n",
        "ts_texts = [test_claims[claim_id]['claim_text'] for claim_id in ts_numbers]\n",
        "\n",
        "print(\"Train claims:\", len(tr_claims))\n",
        "print(\"Dev claims:\", len(dev_claims))   \n",
        "print(\"Evidences:\", len(evidences))\n",
        "print(\"Test claims:\", len(test_claims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ####Local test#####\n",
        "# import os\n",
        "# import json\n",
        "\n",
        "# train_file = \"data/train-claims.json\"\n",
        "# with open(train_file, 'r') as f:\n",
        "#     tr_claims = json.load(f)\n",
        "    \n",
        "\n",
        "# print(\"Train claims:\", len(tr_claims))\n",
        "# id = list(tr_claims.keys())[0]\n",
        "# print(\"Train claim id:\", id)\n",
        "# tr_claim_test = tr_claims[id]\n",
        "# tr_claim_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "######build vocab for train and evidence######\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm \n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield word_tokenize(text.lower())\n",
        "\n",
        "# Build vocabulary manually using Counter\n",
        "def build_vocab_from_iterator(token_iterator, min_freq, specials_tokens=('<cls>', '<unk>', '<pad>')):\n",
        "    counter = Counter()\n",
        "    for tokens in token_iterator:\n",
        "        counter.update(tokens)\n",
        "\n",
        "    vocab = {special_token: idx for idx, special_token in enumerate(specials_tokens)}\n",
        "    current_index = len(specials_tokens)\n",
        "    for token, freq in counter.items():\n",
        "        if freq >= min_freq: # only include tokens with frequency >= min_freq\n",
        "            vocab[token] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "    stoi = vocab\n",
        "    itos = {idx: token for token, idx in stoi.items()}\n",
        "    return vocab, stoi, itos\n",
        "\n",
        "# Create the vocab for train and evidence\n",
        "# vocab, string_to_index, index_to_string = build_vocab_from_iterator(yield_tokens(tr_texts + evidences_texts), min_freq=5)\n",
        "# print(\"Vocab size:\", len(vocab))\n",
        "\n",
        "# with open(\"vocab.json\", \"w\") as f:\n",
        "#     json.dump(vocab, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 112508\n"
          ]
        }
      ],
      "source": [
        "with open(\"vocab.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "print(\"Vocab size:\", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1208827/1208827 [01:18<00:00, 15325.05it/s]\n"
          ]
        }
      ],
      "source": [
        "def text_to_indices(text, vocab):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    indices = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
        "    return [vocab[\"<cls>\"]] + indices\n",
        "\n",
        "tr_text_indices = [text_to_indices(text, vocab) for text in tr_texts]\n",
        "dev_text_indices = [text_to_indices(text, vocab) for text in dev_texts]\n",
        "ts_text_indices = [text_to_indices(text, vocab) for text in ts_texts]\n",
        "\n",
        "# evidences_text_indices = [text_to_indices(text, vocab) for text in tqdm(evidences_texts)]\n",
        "\n",
        "# with open(\"evidences_text_indices.json\", \"w\") as f:\n",
        "#     json.dump(evidences_text_indices, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidences text indices: 1208827\n"
          ]
        }
      ],
      "source": [
        "with open(\"evidences_text_indices.json\", \"r\") as f:\n",
        "    evidences_text_indices = json.load(f)\n",
        "print(\"Evidences text indices:\", len(evidences_text_indices))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Salist\n",
            "[nltk_data]     desk2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to C:\\Users\\Salist\n",
            "[nltk_data]     desk2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def word_tokenize_and_lemmatize(text):\n",
        "    def lemmatize_word(token):\n",
        "        token = lemmatizer.lemmatize(token, pos='v')\n",
        "        token = lemmatizer.lemmatize(token, pos='n') if token != token else token\n",
        "        return token\n",
        "    \n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
        "    lemmed_tokens = [lemmatize_word(token) for token in tokens]\n",
        "    return lemmed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ####Local test#####\n",
        "# word_tokenize_and_lemmatize(tr_claim_test['claim_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from scipy import sparse\n",
        "# import joblib\n",
        "\n",
        "# evidence_ids = evidences.keys()\n",
        "# evidence_text_list = [evidences[eid] for eid in evidence_ids]\n",
        "# evidence_vectorizer = TfidfVectorizer(tokenizer=word_tokenize_and_lemmatize, max_features=5000)\n",
        "# evidence_vectors = evidence_vectorizer.fit_transform(evidence_text_list)\n",
        "\n",
        "# sparse.save_npz(\"evidence_vectors.npz\", evidence_vectors)\n",
        "# joblib.dump(evidence_vectorizer, \"evidence_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy import sparse\n",
        "import joblib\n",
        "\n",
        "def save_tfidf_model(text_list, path_name):\n",
        "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize_and_lemmatize, max_features=5000)\n",
        "    text_ids = list(text_list.keys())\n",
        "    text_list = [text_list[id] for id in text_ids]\n",
        "\n",
        "    tfidf_matrix = vectorizer.fit_transform(text_list)\n",
        "    vector_path=str(path_name + \"_vectors.npz\")\n",
        "    model_path=str(path_name + \"_vectorizer.pkl\")\n",
        "    sparse.save_npz(vector_path, tfidf_matrix)\n",
        "    joblib.dump(vectorizer, model_path)\n",
        "    print(f\"Saved TF-IDF vectors to '{vector_path}' and vectorizer to '{model_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#######RUN ON COLAB##########\n",
        "# save_tfidf_model(evidences, \"evidences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded TF-IDF matrix from 'evidences_vectors.npz' and vectorizer from 'evidences_vectorizer.pkl'\n"
          ]
        }
      ],
      "source": [
        "from scipy import sparse\n",
        "import joblib\n",
        "\n",
        "def load_tfidf(path_name):\n",
        "    vector_path=str(path_name + \"_vectors.npz\")\n",
        "    model_path=str(path_name + \"_vectorizer.pkl\")\n",
        "    tfidf_matrix = sparse.load_npz(vector_path)\n",
        "    vectorizer = joblib.load(model_path)\n",
        "    print(f\"Loaded TF-IDF matrix from '{vector_path}' and vectorizer from '{model_path}'\")\n",
        "    return tfidf_matrix, vectorizer\n",
        "\n",
        "evidence_vectors, evidence_vectorizer = load_tfidf(\"evidences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from scipy import sparse\n",
        "# import joblib\n",
        "# # sparse.save_npz(\"evidence_vectors.npz\", evidence_vectors)\n",
        "# evidence_vectors = sparse.load_npz(\"evidence_vectors.npz\")\n",
        "# evidence_vectorizer = joblib.load(\"evidence_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class Retrival_Encode_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.1):\n",
        "        super(Retrival_Encode_LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<pad>'])\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "\n",
        "    def forward(self, x, real_len):\n",
        "        embedded = self.embedding(x)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, real_len, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, (lstm_hidden, _) = self.lstm(packed)\n",
        "        hidden_forward = lstm_hidden[-2, :, :]\n",
        "        hidden_backward = lstm_hidden[-1, :, :]\n",
        "        hidden_cat = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
        "        output_embedding = self.fc(hidden_cat)\n",
        "        return output_embedding\n",
        "\n",
        "\n",
        "class Retrival_Encode_GRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.1):\n",
        "        super(Retrival_Encode_GRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<pad>'])\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim) #bidirectional\n",
        "\n",
        "    def forward(self, x, real_len):\n",
        "        embedded = self.embedding(x)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, real_len, batch_first=True, enforce_sorted=False)\n",
        "        output, hidden = self.gru(packed)\n",
        "        hidden_forward = hidden[-2, :, :]\n",
        "        hidden_backward = hidden[-1, :, :]\n",
        "        hidden_cat = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
        "        output_embedding = self.fc(hidden_cat)\n",
        "        return output_embedding\n",
        "\n",
        "#test generate_batched_embeddings\n",
        "def generate_batched_embeddings_ts(model, evidences_text_indices, batch_size=32):\n",
        "    all_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(evidences_text_indices), batch_size)):\n",
        "            batch_evi_indices = evidences_text_indices[i:i + batch_size]\n",
        "            real_len = [len(indices) for indices in batch_evi_indices]\n",
        "            batch_evi_indices_pad = torch.nn.utils.rnn.pad_sequence([torch.tensor(indices) for indices in batch_evi_indices], batch_first=True, padding_value=vocab['<pad>'])\n",
        "            # batch_embeddings = model(evidence_batch_indices.cuda(), real_len)\n",
        "            batch_embeddings = model(batch_evi_indices_pad, real_len)\n",
        "\n",
        "            all_embeddings.extend(batch_embeddings.tolist())\n",
        "            del batch_embeddings\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return torch.tensor(all_embeddings)\n",
        "\n",
        "#returns f1 for each claim\n",
        "def evaluate_retrival(claims, top_evidence_id):\n",
        "    claims_f1 = []\n",
        "    for i, claim_id in enumerate(claims.keys()):\n",
        "        correct = 0\n",
        "        recall = 0.0\n",
        "        precision = 0.0\n",
        "        fscore = 0.0\n",
        "        \n",
        "        claim = claims[claim_id]\n",
        "        true_evidence_ids = claim['evidences']\n",
        "        for true_evidence in true_evidence_ids:\n",
        "            true_evidence_id = int(true_evidence.split('-')[1])\n",
        "            if true_evidence_id in top_evidence_id:\n",
        "                correct += 1\n",
        "        if correct > 0:\n",
        "            recall = correct / len(true_evidence_ids)\n",
        "            precision = correct / len(top_evidence_id[i])\n",
        "            fscore = (2 * precision * recall) / (precision + recall)\n",
        "        claims_f1.append(fscore)\n",
        "    return claims_f1\n",
        "\n",
        "#returns avg f1 for input claims\n",
        "def calc_f1(claim_texts_indices, claims, evidences_text_indices, model, bathch_size=32):\n",
        "    model.eval()\n",
        "\n",
        "    claim_embeddings = generate_batched_embeddings_ts(model, claim_texts_indices, batch_size=bathch_size)\n",
        "    claim_embeddings_norm = F.normalize(claim_embeddings, p=2, dim=1)\n",
        "\n",
        "    evidences_embeddings = generate_batched_embeddings_ts(model, evidences_text_indices, batch_size=bathch_size)\n",
        "    evidences_embeddings_norm = F.normalize(evidences_embeddings, p=2, dim=1)\n",
        "\n",
        "    cos_similarities = torch.matmul(claim_embeddings_norm, evidences_embeddings_norm.T)\n",
        "\n",
        "    top_k = 5\n",
        "    top_k_indices = torch.topk(cos_similarities, top_k, dim=1).indices\n",
        "    top_k_indices = top_k_indices.numpy()\n",
        "\n",
        "    claims_f1 = evaluate_retrival(claims, top_k_indices)\n",
        "    model.train()\n",
        "    return np.mean(claims_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train generate_text_embeddings\n",
        "def generate_text_embeddings_tr(model, text_indices):  \n",
        "  real_len = [len(indices) for indices in text_indices]\n",
        "  pad_text_indices = torch.nn.utils.rnn.pad_sequence(\n",
        "      [torch.tensor(indices) for indices in text_indices],\n",
        "        batch_first=True, padding_value=vocab['<pad>'])\n",
        "\n",
        "#   embeddings = model(pad_text_indices.cuda(), real_len)\n",
        "  embeddings = model(pad_text_indices, real_len)\n",
        "  embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
        "  return embeddings_norm\n",
        "\n",
        "#returns all evidence indices included in the claim tr\n",
        "def get_real_evidences(batch_claims):\n",
        "    evi_indices = []\n",
        "    claims_pos_evidence_indices = []\n",
        "    for claim in batch_claims:\n",
        "        pos_evi_indices = []\n",
        "        for evi_number in claim[\"evidences\"]:\n",
        "            cur_evi_index = evi_number_to_evi_id[evi_number]\n",
        "            if cur_evi_index not in evi_indices:\n",
        "                evi_indices.append(cur_evi_index)\n",
        "            pos_evi_indices.append(evi_indices.index(cur_evi_index))\n",
        "        claims_pos_evidence_indices.append(pos_evi_indices)\n",
        "    return evi_indices, claims_pos_evidence_indices\n",
        "\n",
        "#loss based on cosine similarity of pos and neg evidence\n",
        "def contrastive_loss(claim_embedding, pos_evi_embeddings, neg_evi_embeddings, temperature=0.1):\n",
        "    \"\"\"Compute contrastive loss for a claim\"\"\"\n",
        "    pos_sim = torch.exp(torch.matmul(claim_embedding, pos_evi_embeddings.T) / temperature).sum()\n",
        "    neg_sim = torch.exp(torch.matmul(claim_embedding, neg_evi_embeddings.T) / temperature).sum()\n",
        "\n",
        "    loss = -torch.log(pos_sim / (pos_sim + neg_sim))\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Retrival_Encode_GRU' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m test_period \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[0;32m     13\u001b[0m log_period \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m---> 15\u001b[0m model\u001b[38;5;241m=\u001b[39m Retrival_Encode_GRU(\u001b[38;5;28mlen\u001b[39m(vocab), embedding_dim, hidden_dim, num_layers, dropout)\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# model = model.cuda()\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Retrival_Encode_GRU' is not defined"
          ]
        }
      ],
      "source": [
        "#########RUN ON COLAB##########\n",
        "########Training Model##########\n",
        "import random\n",
        "max_epochs = 10  ######Change Later######\n",
        "max_steps = 100  ######Change Later######\n",
        "batch_size = 64\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "learning_rate = 0.001\n",
        "test_period = 40\n",
        "log_period = 20\n",
        "\n",
        "model= Retrival_Encode_GRU(len(vocab), embedding_dim, hidden_dim, num_layers, dropout)\n",
        "model = model\n",
        "# model = model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "step = 0\n",
        "model.train()\n",
        "max_f1 = 0.0\n",
        "for epoch in range(max_epochs):\n",
        "    if step > max_steps:\n",
        "        break\n",
        "    print(f\"Epoch {epoch+1}/{max_epochs}\")\n",
        "    random.shuffle(tr_numbers)\n",
        "\n",
        "    for i in range(0, len(tr_numbers), batch_size):\n",
        "        if step > max_steps:\n",
        "            break\n",
        "        step += 1\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Step {step}\")\n",
        "        batch_claim_ids = tr_numbers[i:i + batch_size] # claim ids\n",
        "        batch_claims = [tr_claims[claim_id] for claim_id in batch_claim_ids]\n",
        "        batch_tr_indices = [claim_number_to_tr_id[claim_id] for claim_id in batch_claim_ids] #claims' train indices\n",
        "\n",
        "        claim_text_indices = [tr_text_indices[i] for i in batch_tr_indices]\n",
        "        norm_claim_embeddings = generate_text_embeddings_tr(model, claim_text_indices)\n",
        "\n",
        "        ##Todo modify positive and negative evidence list##\n",
        "        all_real_evi, pos_evi = get_real_evidences(batch_claims)\n",
        "        cur_evi_indices = [evidences_text_indices[id] for id in all_real_evi]\n",
        "        norm_evi_embeddings = generate_text_embeddings_tr(model, cur_evi_indices)\n",
        "\n",
        "        loss = []\n",
        "        for i, claim_embedding in enumerate(norm_claim_embeddings):\n",
        "            pos_evi_embedding = norm_evi_embeddings[torch.tensor(pos_evi[i])]\n",
        "            # neg_evidences = [j for j in range(len(all_real_evi)) if j not in pos_evi[i]] #some postive evidence of oter claims\n",
        "            # neg_evi_embedding = norm_evi_embeddings[torch.tensor(neg_evidences)]\n",
        "            neg_evi_embedding = norm_evi_embeddings[torch.tensor([j for j in range(len(all_real_evi)) if j not in pos_evi[i]])]\n",
        "\n",
        "            loss.append(contrastive_loss(claim_embedding, pos_evi_embedding, neg_evi_embedding))\n",
        "        \n",
        "        loss = torch.mean(torch.stack(loss))\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "        if step % log_period == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.item()}\")\n",
        "        if step % test_period == 0:\n",
        "            print(\"Evaluating on dev set...\")\n",
        "            dev_f1 = calc_f1(dev_text_indices, dev_claims, evidences_text_indices, model)\n",
        "            avg_f1 = np.mean(dev_f1)\n",
        "            print(f\"Avg F1 on dev set: {avg_f1}\")\n",
        "            if avg_f1 > max_f1:\n",
        "                max_f1 = avg_f1\n",
        "                print(f\"New best F1: {max_f1}, model saved.\")\n",
        "                torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'load_tfidf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m evidences_vectors, evidences_vectorizer \u001b[38;5;241m=\u001b[39m load_tfidf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevidences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m dev_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dev_claims\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      6\u001b[0m dev_claims_vectors \u001b[38;5;241m=\u001b[39m evidences_vectorizer\u001b[38;5;241m.\u001b[39mtransform([dev_claims[\u001b[38;5;28mid\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaim_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dev_ids])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'load_tfidf' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "evidences_vectors, evidences_vectorizer = load_tfidf(\"evidences\")\n",
        "dev_ids = list(dev_claims.keys())\n",
        "dev_claims_vectors = evidences_vectorizer.transform([dev_claims[id]['claim_text'] for id in dev_ids])\n",
        "\n",
        "def top_k_evidence(vectors, k=5):\n",
        "    similarity = cosine_similarity(vectors, evidences_vectors)\n",
        "    top_k_indices = np.argsort(similarity, axis=1)[:, -k:]\n",
        "    top_k_indices = np.flip(top_k_indices, axis=1)\n",
        "    return top_k_indices\n",
        "\n",
        "dev_top5_evidence_id = top_k_evidence(dev_claims_vectors, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev claims F1 score: 0.08017419088847662\n"
          ]
        }
      ],
      "source": [
        "def evaluate_retrival(claims, top_evidence_id):\n",
        "    evidence_f1 = []\n",
        "    for i, claim_id in enumerate(claims.keys()):\n",
        "        correct = 0\n",
        "        recall = 0.0\n",
        "        precision = 0.0\n",
        "        fscore = 0.0\n",
        "        \n",
        "        claim = claims[claim_id]\n",
        "        true_evidence_ids = claim['evidences']\n",
        "        for true_evidence in true_evidence_ids:\n",
        "            true_evidence_id = int(true_evidence.split('-')[1])\n",
        "            if true_evidence_id in top_evidence_id:\n",
        "                correct += 1\n",
        "        if correct > 0:\n",
        "            recall = correct / len(true_evidence_ids)\n",
        "            precision = correct / len(top_evidence_id[i])\n",
        "            fscore = (2 * precision * recall) / (precision + recall)\n",
        "        evidence_f1.append(fscore)\n",
        "    return evidence_f1\n",
        "\n",
        "dev_top5_evidence_f1 = evaluate_retrival(dev_claims, dev_top5_evidence_id)\n",
        "print(\"Dev claims F1 score:\", np.mean(dev_top5_evidence_f1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "GroupID44_comp90042_project_2025.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
