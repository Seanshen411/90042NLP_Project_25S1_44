{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############Colab Parameters###########\n",
        "TFIDF_max_features = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train claims: 1228\n",
            "Dev claims: 154\n",
            "Evidences: 1208827\n",
            "Test claims: 153\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "data_file = \"/content/data\"\n",
        "data_file = \"data\"\n",
        "train_file = os.path.join(data_file, 'train-claims.json')\n",
        "dev_file = os.path.join(data_file, 'dev-claims.json')\n",
        "evidence_file = os.path.join(data_file, 'evidence.json')\n",
        "test_file = os.path.join(data_file, 'test-claims-unlabelled.json')\n",
        "\n",
        "with open(train_file, 'r') as f:\n",
        "    tr_claims = json.load(f)\n",
        "tr_ids = list(tr_claims.keys())\n",
        "tr_texts = [tr_claims[claim_id]['claim_text'] for claim_id in tr_ids]\n",
        "\n",
        "with open(dev_file, 'r') as f:\n",
        "    dev_claims = json.load(f)\n",
        "dev_ids = list(dev_claims.keys())\n",
        "dev_texts = [dev_claims[claim_id]['claim_text'] for claim_id in dev_ids]\n",
        "\n",
        "with open(evidence_file, 'r') as f:\n",
        "    evidences = json.load(f)\n",
        "evidences_ids = list(evidences.keys())\n",
        "evidences_texts = [evidences[evidence_id] for evidence_id in evidences_ids]\n",
        "\n",
        "with open(test_file, 'r') as f:\n",
        "    test_claims = json.load(f)\n",
        "ts_ids = list(test_claims.keys())\n",
        "ts_texts = [test_claims[claim_id]['claim_text'] for claim_id in ts_ids]\n",
        "\n",
        "print(\"Train claims:\", len(tr_claims))\n",
        "print(\"Dev claims:\", len(dev_claims))   \n",
        "print(\"Evidences:\", len(evidences))\n",
        "print(\"Test claims:\", len(test_claims))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ####Local test#####\n",
        "# import os\n",
        "# import json\n",
        "\n",
        "# train_file = \"data/train-claims.json\"\n",
        "# with open(train_file, 'r') as f:\n",
        "#     tr_claims = json.load(f)\n",
        "    \n",
        "\n",
        "# print(\"Train claims:\", len(tr_claims))\n",
        "# id = list(tr_claims.keys())[0]\n",
        "# print(\"Train claim id:\", id)\n",
        "# tr_claim_test = tr_claims[id]\n",
        "# tr_claim_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Salist\n",
            "[nltk_data]     desk2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to C:\\Users\\Salist\n",
            "[nltk_data]     desk2\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def word_tokenize_and_lemmatize(text):\n",
        "    def lemmatize_word(token):\n",
        "        token = lemmatizer.lemmatize(token, pos='v')\n",
        "        token = lemmatizer.lemmatize(token, pos='n') if token != token else token\n",
        "        return token\n",
        "    \n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
        "    lemmed_tokens = [lemmatize_word(token) for token in tokens]\n",
        "    return lemmed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ####Local test#####\n",
        "# word_tokenize_and_lemmatize(tr_claim_test['claim_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from scipy import sparse\n",
        "# import joblib\n",
        "\n",
        "# evidence_ids = evidences.keys()\n",
        "# evidence_text_list = [evidences[eid] for eid in evidence_ids]\n",
        "# evidence_vectorizer = TfidfVectorizer(tokenizer=word_tokenize_and_lemmatize, max_features=5000)\n",
        "# evidence_vectors = evidence_vectorizer.fit_transform(evidence_text_list)\n",
        "\n",
        "# sparse.save_npz(\"evidence_vectors.npz\", evidence_vectors)\n",
        "# joblib.dump(evidence_vectorizer, \"evidence_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy import sparse\n",
        "import joblib\n",
        "\n",
        "def save_tfidf_model(text_list, path_name):\n",
        "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize_and_lemmatize, max_features=5000)\n",
        "    text_ids = list(text_list.keys())\n",
        "    text_list = [text_list[id] for id in text_ids]\n",
        "\n",
        "    tfidf_matrix = vectorizer.fit_transform(text_list)\n",
        "    vector_path=str(path_name + \"_vectors.npz\")\n",
        "    model_path=str(path_name + \"_vectorizer.pkl\")\n",
        "    sparse.save_npz(vector_path, tfidf_matrix)\n",
        "    joblib.dump(vectorizer, model_path)\n",
        "    print(f\"Saved TF-IDF vectors to '{vector_path}' and vectorizer to '{model_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#######RUN ON COLAB##########\n",
        "# save_tfidf_model(evidences, \"evidences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "import joblib\n",
        "\n",
        "def load_tfidf(path_name):\n",
        "    vector_path=str(path_name + \"_vectors.npz\")\n",
        "    model_path=str(path_name + \"_vectorizer.pkl\")\n",
        "    tfidf_matrix = sparse.load_npz(vector_path)\n",
        "    vectorizer = joblib.load(model_path)\n",
        "    print(f\"Loaded TF-IDF matrix from '{vector_path}' and vectorizer from '{model_path}'\")\n",
        "    return tfidf_matrix, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from scipy import sparse\n",
        "# import joblib\n",
        "# # sparse.save_npz(\"evidence_vectors.npz\", evidence_vectors)\n",
        "# evidence_vectors = sparse.load_npz(\"evidence_vectors.npz\")\n",
        "# evidence_vectorizer = joblib.load(\"evidence_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######build vocab for train and evidence######\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm \n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield word_tokenize(text.lower())\n",
        "\n",
        "# Build vocabulary manually using Counter\n",
        "def build_vocab_from_iterator(token_iterator, min_freq, specials_tokens=('<cls>', '<unk>', '<pad>')):\n",
        "    counter = Counter()\n",
        "    for tokens in token_iterator:\n",
        "        counter.update(tokens)\n",
        "\n",
        "    vocab = {special_token: idx for idx, special_token in enumerate(specials_tokens)}\n",
        "    current_index = len(specials_tokens)\n",
        "    for token, freq in counter.items():\n",
        "        if freq >= min_freq: # only include tokens with frequency >= min_freq\n",
        "            vocab[token] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "    stoi = vocab\n",
        "    itos = {idx: token for token, idx in stoi.items()}\n",
        "    return vocab, stoi, itos\n",
        "\n",
        "# Create the vocab for train and evidence\n",
        "vocab, string_to_index, index_to_string = build_vocab_from_iterator(yield_tokens(tr_texts + evidences_texts), min_freq=5)\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "\n",
        "def text_to_indices(text, vocab):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    indices = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
        "    return [vocab[\"<cls>\"]] + indices\n",
        "\n",
        "tr_text_indices = [text_to_indices(text, vocab) for text in tr_texts]\n",
        "dev_text_indices = [text_to_indices(text, vocab) for text in dev_texts]\n",
        "ts_text_indices = [text_to_indices(text, vocab) for text in ts_texts]\n",
        "\n",
        "evidences_text_indices = [text_to_indices(text, vocab) for text in tqdm(evidences_texts)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded TF-IDF matrix from 'evidences_vectors.npz' and vectorizer from 'evidences_vectorizer.pkl'\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "evidences_vectors, evidences_vectorizer = load_tfidf(\"evidences\")\n",
        "dev_ids = list(dev_claims.keys())\n",
        "dev_claims_vectors = evidences_vectorizer.transform([dev_claims[id]['claim_text'] for id in dev_ids])\n",
        "\n",
        "def top_k_evidence(vectors, k=5):\n",
        "    similarity = cosine_similarity(vectors, evidences_vectors)\n",
        "    top_k_indices = np.argsort(similarity, axis=1)[:, -k:]\n",
        "    top_k_indices = np.flip(top_k_indices, axis=1)\n",
        "    return top_k_indices\n",
        "\n",
        "dev_top5_evidence_id = top_k_evidence(dev_claims_vectors, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dev claims F1 score: 0.08017419088847662\n"
          ]
        }
      ],
      "source": [
        "def evaluate_retrival(dev_claims, top_evidence_id):\n",
        "    evidence_f1 = []\n",
        "    for i, claim_id in enumerate(dev_claims.keys()):\n",
        "        correct = 0\n",
        "        recall = 0.0\n",
        "        precision = 0.0\n",
        "        fscore = 0.0\n",
        "        \n",
        "        claim = dev_claims[claim_id]\n",
        "        true_evidence_ids = claim['evidences']\n",
        "        for true_evidence in true_evidence_ids:\n",
        "            true_evidence_id = int(true_evidence.split('-')[1])\n",
        "            if true_evidence_id in top_evidence_id:\n",
        "                correct += 1\n",
        "        if correct > 0:\n",
        "            recall = correct / len(true_evidence_ids)\n",
        "            precision = correct / len(top_evidence_id[i])\n",
        "            fscore = (2 * precision * recall) / (precision + recall)\n",
        "        evidence_f1.append(fscore)\n",
        "    return evidence_f1\n",
        "\n",
        "dev_top5_evidence_f1 = evaluate_retrival(dev_claims, dev_top5_evidence_id)\n",
        "print(\"Dev claims F1 score:\", np.mean(dev_top5_evidence_f1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "GroupID44_comp90042_project_2025.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
